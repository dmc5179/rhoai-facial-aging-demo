{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f4dada-51a8-4f06-9618-0f2deda72bee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Serving \n",
    "\n",
    "In notebook '04-04-accident-recognition' we were able to use the retrained model to predict a 'severe' or 'moderate' car accident within an image.  \n",
    "\n",
    "Now we will determine if we can query the model directly from the model server we have created.  This will be done through an API call.\n",
    "\n",
    "**Important**: First, enter the inference endpoint URL you got after deploying the model server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38338ba-d858-4096-92a1-284d35679005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normally, this should be\n",
    "# RestURL = 'http://modelmesh-serving.userX:8008' , with userX being replaced by the user you have been assigned.\n",
    "# CHANGE the value below, or nothing will work!\n",
    "RestURL = 'https://famodel-sandbox.apps.rhoai.sandbox2386.opentlc.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4c9cc-1e3a-43a4-92cf-44af26573030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you did not use the Workbench image designed for this Lab, you can uncomment and run the following line to install the required packages.\n",
    "#!pip install --upgrade pip\n",
    "!pip install --no-cache-dir --no-dependencies -r requirements.txt\n",
    "!pip install --no-cache-dir --no-dependencies opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930247ab-2e10-4db4-888c-517ca3a4cec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "## note that to keep the notebook readable, a lot of code was moved into the file remote_infer.py.\n",
    "## if you are curious, open it to see the various functions required around this prediction\n",
    "from remote_infer import process_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e84546-979c-4dc2-9fc7-21d997001beb",
   "metadata": {},
   "source": [
    "## We will define the inference URL, the model name, the YAML file with your classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ac64a-76a4-4630-8da3-0174fbdf54cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "infer_url = f'{RestURL}/v2/models/famodel/versions/1/infer'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d67e4-1933-4957-80fd-c4b143d14279",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Now we set the parameters for the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31725659-710d-4fdc-b195-4d56b299f419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. The image you want to analyze\n",
    "image_path = '/opt/app-root/src/data/16.png'  # You can replace this with an image you upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326d21db-0f7b-4f89-86bd-84f4c04b7c44",
   "metadata": {},
   "source": [
    "## Launch the inference and show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15e0ccd-5c65-4dc7-80bc-2e583223ac59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "from transformers import ViTImageProcessor\n",
    "import warnings\n",
    "\n",
    "# Suppress the InsecureRequestWarning for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", message=\"Unverified HTTPS request is being made.*\")\n",
    "\n",
    "# --- 1. SET UP YOUR VARIABLES ---\n",
    "\n",
    "# The base URL for your deployed model server\n",
    "RestURL = 'https://famodel-sandbox.apps.rhoai.sandbox2386.opentlc.com'\n",
    "\n",
    "# The full URL for your specific model and version\n",
    "INFERENCE_URL = f'{RestURL}/v2/models/famodel/versions/1/infer'\n",
    "\n",
    "# This dictionary maps the model's output index to the age group label.\n",
    "id2label = {0: '01', 1: '02', 2: '03', 3: '04', 4: '05', 5: '06-07', 6: '08-09', 7: '10-12', 8: '13-15', 9: '16-20', 10: '21-25', 11: '26-30', 12: '31-35', 13: '36-40', 14: '41-45', 15: '46-50', 16: '51-55', 17: '56-60', 18: '61-65', 19: '66-70', 20: '71-80', 21: '81-90', 22: '90+'}\n",
    "\n",
    "# --- THIS IS THE CORRECTED LINE ---\n",
    "# Load the original processor config from the Hugging Face Hub\n",
    "path_to_processor = \"dima806/facial_age_image_detection\"\n",
    "\n",
    "# The path to an image you want to test\n",
    "image_path = '/opt/app-root/src/data/16.png' \n",
    "\n",
    "# --- 2. LOAD AND PREPROCESS YOUR IMAGE ---\n",
    "\n",
    "print(f\"Loading and preparing image: {image_path}\")\n",
    "try:\n",
    "    # Load the processor to prepare the image exactly as the model expects\n",
    "    processor = ViTImageProcessor.from_pretrained(path_to_processor)\n",
    "    \n",
    "    # Load the test image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    display(image)\n",
    "\n",
    "    # The processor handles resizing, normalization, etc.\n",
    "    inputs = processor(images=image, return_tensors=\"np\")\n",
    "    image_data = inputs['pixel_values'].astype(np.float32)\n",
    "\n",
    "    # --- 3. BUILD THE CORRECT PAYLOAD ---\n",
    "    \n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"pixel_values\",\n",
    "                \"shape\": image_data.shape,\n",
    "                \"datatype\": \"FP32\",\n",
    "                \"data\": image_data.tolist()\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # --- 4. SEND THE REQUEST AND GET THE RESULT ---\n",
    "\n",
    "    print(f\"Sending request to the model server...\")\n",
    "    response = requests.post(INFERENCE_URL, json=payload, verify=False)\n",
    "    response.raise_for_status() \n",
    "    \n",
    "    result = response.json()\n",
    "    \n",
    "    output_data = np.array(result['outputs'][0]['data'])\n",
    "    predicted_class_index = np.argmax(output_data)\n",
    "    predicted_age_group = id2label[predicted_class_index]\n",
    "    \n",
    "    print(\"\\n✅ Success!\")\n",
    "    print(f\"Predicted Age Group: {predicted_age_group}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: The image file was not found at '{image_path}'. Please check the path and try again.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ An error occurred during inference: {e}\")\n",
    "    if 'response' in locals() and hasattr(response, 'text'):\n",
    "        print(\"--- Server Response ---\")\n",
    "        print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
